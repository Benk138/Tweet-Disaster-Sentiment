{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nThe purpose of this competition is to try to predict, given a tweet, whether that tweet is announcing a disaster or not. In order to do this, we use will use an ensemble of the DistilBERT model and the XLNet model.","metadata":{}},{"cell_type":"markdown","source":"# Importing dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, XLNetTokenizer, TFXLNetForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2023-09-25T21:29:35.286580Z","iopub.execute_input":"2023-09-25T21:29:35.287506Z","iopub.status.idle":"2023-09-25T21:29:35.292956Z","shell.execute_reply.started":"2023-09-25T21:29:35.287469Z","shell.execute_reply":"2023-09-25T21:29:35.291662Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-25T21:29:35.295120Z","iopub.execute_input":"2023-09-25T21:29:35.295443Z","iopub.status.idle":"2023-09-25T21:29:35.369191Z","shell.execute_reply.started":"2023-09-25T21:29:35.295413Z","shell.execute_reply":"2023-09-25T21:29:35.368015Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2023-09-25T21:29:35.371265Z","iopub.execute_input":"2023-09-25T21:29:35.371950Z","iopub.status.idle":"2023-09-25T21:29:35.388406Z","shell.execute_reply.started":"2023-09-25T21:29:35.371917Z","shell.execute_reply":"2023-09-25T21:29:35.387143Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"         id keyword location  \\\n0         1     NaN      NaN   \n1         4     NaN      NaN   \n2         5     NaN      NaN   \n3         6     NaN      NaN   \n4         7     NaN      NaN   \n...     ...     ...      ...   \n7608  10869     NaN      NaN   \n7609  10870     NaN      NaN   \n7610  10871     NaN      NaN   \n7611  10872     NaN      NaN   \n7612  10873     NaN      NaN   \n\n                                                   text  target  \n0     Our Deeds are the Reason of this #earthquake M...       1  \n1                Forest fire near La Ronge Sask. Canada       1  \n2     All residents asked to 'shelter in place' are ...       1  \n3     13,000 people receive #wildfires evacuation or...       1  \n4     Just got sent this photo from Ruby #Alaska as ...       1  \n...                                                 ...     ...  \n7608  Two giant cranes holding a bridge collapse int...       1  \n7609  @aria_ahrary @TheTawniest The out of control w...       1  \n7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n7611  Police investigating after an e-bike collided ...       1  \n7612  The Latest: More Homes Razed by Northern Calif...       1  \n\n[7613 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>10869</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>10870</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>10871</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>10872</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>10873</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Importing the tokenizers for DistilBERT and XLNet","metadata":{}},{"cell_type":"code","source":"tokenizer_B = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmax_length = 140\n\ndef encode_text(df, tokenizer, max_length):\n    return tokenizer.batch_encode_plus(\n        df['text'].tolist(),\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors='tf'\n    )\n\ntrain_data_B = encode_text(train_df, tokenizer_B, max_length)\ntest_data_B = encode_text(test_df, tokenizer_B, max_length)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T21:29:35.390198Z","iopub.execute_input":"2023-09-25T21:29:35.390629Z","iopub.status.idle":"2023-09-25T21:29:46.206918Z","shell.execute_reply.started":"2023-09-25T21:29:35.390588Z","shell.execute_reply":"2023-09-25T21:29:46.205789Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"tokenizer_X = XLNetTokenizer.from_pretrained('xlnet-base-cased')\ntrain_data_X = encode_text(train_df, tokenizer_X, max_length)\ntest_data_X = encode_text(test_df, tokenizer_X, max_length)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T21:29:46.210412Z","iopub.execute_input":"2023-09-25T21:29:46.210931Z","iopub.status.idle":"2023-09-25T21:29:50.363519Z","shell.execute_reply.started":"2023-09-25T21:29:46.210893Z","shell.execute_reply":"2023-09-25T21:29:50.362515Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model_B = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\nmodel_X = TFXLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T21:29:50.364932Z","iopub.execute_input":"2023-09-25T21:29:50.365240Z","iopub.status.idle":"2023-09-25T21:29:55.901070Z","shell.execute_reply.started":"2023-09-25T21:29:50.365212Z","shell.execute_reply":"2023-09-25T21:29:55.899777Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  warnings.warn(\nSome layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size_B = 10\nepochs_B = 5\nbatch_size_X = 32\nepochs_X = 3","metadata":{"execution":{"iopub.status.busy":"2023-09-25T21:29:55.902844Z","iopub.execute_input":"2023-09-25T21:29:55.903290Z","iopub.status.idle":"2023-09-25T21:29:55.909212Z","shell.execute_reply.started":"2023-09-25T21:29:55.903245Z","shell.execute_reply":"2023-09-25T21:29:55.908035Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Training the models","metadata":{}},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': train_data_B['input_ids'], 'attention_mask': train_data_B['attention_mask']},\n    train_df['target'].values\n)).shuffle(len(train_df)).batch(batch_size_B)\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel_B.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n\nmodel_B.fit(train_dataset, epochs=epochs_B)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T21:29:55.911023Z","iopub.execute_input":"2023-09-25T21:29:55.911992Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n762/762 [==============================] - 3237s 4s/step - loss: 0.4310 - accuracy: 0.8099\nEpoch 2/5\n 38/762 [>.............................] - ETA: 50:46 - loss: 0.2933 - accuracy: 0.8895","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset_X = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': train_data_X['input_ids'], 'attention_mask': train_data_X['attention_mask']},\n    train_df['target'].values\n)).shuffle(len(train_df)).batch(batch_size_X)\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel_X.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n\nmodel_X.fit(train_dataset_X, epochs=epochs_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_B = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': test_data_B['input_ids'], 'attention_mask': test_data_B['attention_mask']}\n)).batch(batch_size_B)\n\nlogits1 = model_B.predict(test_dataset_B).logits\n\n# Convert logits to class predictions\npredictions1 = tf.argmax(logits1, axis=1).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset_X = tf.data.Dataset.from_tensor_slices((\n    {'input_ids': test_data_X['input_ids'], 'attention_mask': test_data_X['attention_mask']}\n)).batch(batch_size_X)\n\n# Get the prediction logits for both models\nlogits2 = model_X.predict(test_dataset_X).logits\n\npredictions2 = tf.argmax(logits2, axis=1).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making the final predictions and then generating the submission file","metadata":{}},{"cell_type":"code","source":"final_predictions = (predictions1 + predictions2) > 1  # This will work since 0 (not disaster) + 0 = 0, 1 (disaster) + 1 = 2\nfinal_predictions = final_predictions.astype(int)\n\nsubmission = pd.DataFrame({'id': test_df['id'], 'target': final_predictions})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few things I would like to try in the future:\n* Try using a larger model: both of the models selected here were moreso selected due to their relative smallness, and I wanted to play around a bit with transformer models on an interesting task\n* I would also like to try and train each of the models for longer in order to have better accuracy on the training data. In different modifications of the batch size with the models, I was never able to get the accuracy to be above around 96% or so on the training data. I think this would be fixed by training longer, as each epoch the accuracy improved quite a bit.\n* This ensemble structure performed much better than the baseline linear regression model that I previously implemented, but I am also curious to try making an architecture from scratch to see how well it could do, instead of the nearly 100 million parameters in these pre-trained models\n* Lastly, I think it could be interesting to try to work on this problem in more languages than English, and then try to apply data augmentation after acquiring other tweets in other languages. For example, all of the tweets in the data set here are in English, but there are pre-trained models trained on large corpuses of non-English text, too and it could be interesting if the model does better on some languages than others!","metadata":{}}]}